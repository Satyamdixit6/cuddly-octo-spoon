# CUDA 3D Convolution vs PyTorch nn.Conv3d Benchmark

This project implements a custom 3D convolution layer using CUDA and C++ PyTorch extensions, and compares its performance (speed, memory, latency) against PyTorch's native `torch.nn.Conv3d`.

The primary goal is to illustrate the process of creating custom CUDA kernels for PyTorch and benchmarking them. The provided custom CUDA kernel is a **simplified direct convolution** and, as seen in the benchmarks, is not expected to outperform PyTorch's `nn.Conv3d`, which relies on highly optimized libraries like cuDNN. Achieving superior performance would require implementing advanced optimization strategies using libraries like CUTLASS.

## Project Structure

cuda_pytorch_conv3d_comparison/
├── conv3d_custom_cuda/           # Source for the custom CUDA operator
│   ├── conv3d_cuda_kernel.cu   # The CUDA kernel for 3D convolution
│   ├── conv3d_cuda.cpp         # C++ bindings for PyTorch (using Pybind11)
│   ├── setup.py                # Script to compile the CUDA extension
│   └── conv3d_custom_cuda_op/  # Directory created during build for the .so file
│       ├── init.py
│       └── _C.cpython-XYZ.so   # Compiled extension
├── models.py                     # PyTorch nn.Module wrappers for both conv layers
├── benchmark.py                  # Script to run benchmarks and compare layers
├── README.md                     # This file
└── post.txt                      # Draft for a social media post


## Theoretical Part: 3D Convolutions

### What is a 3D Convolution?

A 3D convolution operation applies a filter (or kernel) across a 3D input volume. It's an extension of 2D convolutions, which are fundamental to image processing and computer vision. In 3D convolutions, the kernel slides across the input data in three dimensions (depth, height, width), computing the dot product between the kernel entries and the input values it overlaps at each position.

**Input:** A 5D tensor, typically with shape `(N, C_in, D_in, H_in, W_in)`
* `N`: Batch size
* `C_in`: Number of input channels
* `D_in, H_in, W_in`: Depth, Height, Width of the input volume

**Kernel/Filter:** A 5D tensor, typically with shape `(C_out, C_in, K_d, K_h, K_w)`
* `C_out`: Number of output channels (number of distinct filters)
* `C_in`: Number of input channels (must match input)
* `K_d, K_h, K_w`: Depth, Height, Width of the kernel

**Output:** A 5D tensor, with shape `(N, C_out, D_out, H_out, W_out)`
The output dimensions `D_out, H_out, W_out` depend on the input dimensions, kernel size, stride, and padding:
$$ D_{out} = \lfloor \frac{D_{in} + 2P_d - K_d}{S_d} \rfloor + 1 $$
$$ H_{out} = \lfloor \frac{H_{in} + 2P_h - K_h}{S_h} \rfloor + 1 $$
$$ W_{out} = \lfloor \frac{W_{in} + 2P_w - K_w}{S_w} \rfloor + 1 $$
Where `P` is padding and `S` is stride.

Each output channel `C_out` is generated by convolving all `C_in` input channels with a corresponding set of 3D filters and summing the results (plus an optional bias term).

### Use Cases in Vision and Beyond:
* **Video Analysis:** Analyzing sequences of frames where temporal information (depth) is crucial (e.g., action recognition, video object detection).
* **Medical Imaging:** Processing volumetric data like MRI or CT scans for tasks like tumor detection or segmentation.
* **Volumetric Data Processing:** Any domain dealing with 3D grid-like data (e.g., physics simulations, fluid dynamics).

## Custom CUDA Layer

### CUDA Kernel (`conv3d_cuda_kernel.cu`)
The provided kernel implements a straightforward direct 3D convolution. Each CUDA thread computes one output element by iterating through input channels and filter dimensions. Padding and stride are handled in input coordinate calculations. This is an illustrative implementation.

### Role of CUTLASS for High Optimization
**CUTLASS** (CUDA Templates for Linear Algebra Subroutines) is a NVIDIA library providing templated C++ primitives for high-performance GEMM (General Matrix Multiply) and related computations. Convolutions can often be formulated as GEMM operations (e.g., via `im2col` or implicitly).
To highly optimize with CUTLASS, one would:
1.  **Choose a GEMM Strategy:** Either `im2col + GEMM` or an implicit GEMM approach.
2.  **Leverage CUTLASS Features:** Utilize tiled GEMMs, Tensor Cores for mixed-precision, epilogue operations for fusion (bias, activations), and optimized data layouts. This requires deep knowledge of CUTLASS templates and CUDA.

## Compilation and Running

Follow these steps carefully:

1.  **Navigate to the custom operator directory:**
    ```bash
    cd cuda_pytorch_conv3d_comparison/conv3d_custom_cuda/
    ```

2.  **Clean previous build artifacts (optional, but good for a fresh start):**
    ```bash
    rm -rf build/ conv3d_custom_cuda_op/ conv3d_custom_cuda_op.egg-info/
    find . -name '*.so' -delete # If any .so files are directly in this dir
    ```

3.  **Create the target package directory for the compiled extension:**
    ```bash
    mkdir conv3d_custom_cuda_op
    touch conv3d_custom_cuda_op/__init__.py
    ```

4.  **Activate your Python virtual environment** (e.g., `(torchevc)`).

5.  **Compile the extension using `setup.py`****:**
    ```bash
    python setup.py build_ext --inplace
    ```
    Ensure this command completes without errors. It will place the compiled `.so` file inside `conv3d_custom_cuda_op/`.

6.  **Navigate to the project root directory:**
    ```bash
    cd ..
    ```

7.  **Set environment variables for the current terminal session:**
    * **`LD_LIBRARY_PATH`**: To help find PyTorch's shared libraries (like `libc10.so`). Adjust the path if your virtual environment location is different.
        ```bash
        export LD_LIBRARY_PATH="/path/to/your/virtual_env/lib/pythonX.Y/site-packages/torch/lib:$LD_LIBRARY_PATH"
        # Example for your setup:
        # export LD_LIBRARY_PATH="/home/satyam/Music/torchevc/lib/python3.12/site-packages/torch/lib:$LD_LIBRARY_PATH"
        ```
    * **`PYTHONPATH`**: To help Python find your locally compiled custom module.
        ```bash
        export PYTHONPATH="$(pwd)/conv3d_custom_cuda:$PYTHONPATH"
        ```

8.  **Run the benchmark script****:**
    ```bash
    python benchmark.py
    ```

## Benchmarking (`benchmark.py`)

The `benchmark.py` script compares the custom CUDA 3D convolution layer against PyTorch's `nn.Conv3d`.

### What it Measures:
* **Latency:** Average time taken for a single forward pass (in milliseconds).
* **Throughput:** Number of samples (batch size) processed per second.
* **Peak Memory Allocated:** Additional GPU memory (delta) allocated by the layer during its operations (in Megabytes).
* **Correctness:** Compares the custom layer's output against PyTorch's `nn.Conv3d` using the same weights and inputs.

### Interpreting Results:
* **PyTorch `nn.Conv3d`** is expected to be significantly faster and more memory-efficient due to its reliance on cuDNN.
* The **Custom CUDA layer's** performance will reflect its more direct, less optimized implementation. The benchmark output confirms it matches PyTorch numerically.

This benchmark provides a template for evaluating the performance of custom neural network layers.
