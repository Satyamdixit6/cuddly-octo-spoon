{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a3cee96",
      "metadata": {
        "id": "6a3cee96"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "# Add the DiT directory to the Python path\n",
        "sys.path.insert(0, \"/home/satyam/Music/DIT/src/cuda/DiT\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.cpp_extension import load\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from diffusers.models import AutoencoderKL\n",
        "try:\n",
        "    from diffusion.scheduler import create_diffusion  # Adjust based on actual file\n",
        "except ImportError:\n",
        "    # Fallback to diffusers.DDPMScheduler if create_diffusion is unavailable\n",
        "    from diffusers import DDPMScheduler\n",
        "    def create_diffusion(timesteps):\n",
        "        return DDPMScheduler(num_train_timesteps=int(timesteps))\n",
        "import urllib.request\n",
        "\n",
        "# Pre-trained model URLs\n",
        "MODEL_URLS = {\n",
        "    '256': 'https://dl.fbaipublicfiles.com/DiT/models/DiT-XL-2-256x256.pt',\n",
        "    '512': 'https://dl.fbaipublicfiles.com/DiT/models/DiT-XL-2-512x512.pt'\n",
        "}\n",
        "\n",
        "# Load CUDA modules\n",
        "try:\n",
        "    dit_block_mod = load(\n",
        "        name=\"dit_block_mod\",\n",
        "        sources=[\n",
        "            \"/home/satyam/Music/DIT/src/cuda/dit_block.cu\",\n",
        "            \"/home/satyam/Music/DIT/src/cuda/cuda_adaln_modulation.cu\",\n",
        "            \"/home/satyam/Music/DIT/src/cuda/cuda_attention.cu\",\n",
        "            \"/home/satyam/Music/DIT/src/cuda/cuda_mlp.cu\",\n",
        "            \"/home/satyam/Music/DIT/src/cuda/cuda_timestep_embed.cu\",\n",
        "            \"/home/satyam/Music/DIT/src/cuda/cuda_util_kernels.cu\",\n",
        "            \"/home/satyam/Music/DIT/src/cuda/cuda_layernorm.cu\",\n",
        "            \"/home/satyam/Music/DIT/src/cuda/cuda_label_embed.cu\",\n",
        "            \"/home/satyam/Music/DIT/src/cuda/final_layer.cu\"\n",
        "        ],\n",
        "        extra_cuda_cflags=[\"-O3\"],\n",
        "        verbose=True\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load CUDA extensions: {e}\")\n",
        "    raise\n",
        "\n",
        "class DiTInference(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=32,  # 256/8 = 32 for 256x256 images\n",
        "        hidden_size=1152,  # DiT-XL/2 default\n",
        "        depth=28,  # DiT-XL/2 default\n",
        "        num_heads=16,  # DiT-XL/2 default\n",
        "        mlp_ratio=4.0,\n",
        "        num_classes=1000,\n",
        "        learn_sigma=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.learn_sigma = learn_sigma\n",
        "        self.num_classes = num_classes\n",
        "        self.hidden_size = hidden_size\n",
        "        self.depth = depth\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Input embedding\n",
        "        self.x_embedder = nn.Conv2d(4, hidden_size, kernel_size=1)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, input_size ** 2, hidden_size))\n",
        "\n",
        "        # Initialize weights for DiT blocks (loaded from checkpoint later)\n",
        "        self.blocks = []\n",
        "        for i in range(depth):\n",
        "            block_params = {\n",
        "                'hidden_size': hidden_size,\n",
        "                'num_heads': num_heads,\n",
        "                'mlp_ratio': mlp_ratio,\n",
        "                'qkv_bias': True\n",
        "            }\n",
        "            self.blocks.append(block_params)\n",
        "\n",
        "        # Final layer norm and output projection\n",
        "        self.final_layer = nn.Linear(hidden_size, 8 if learn_sigma else 4)\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialize transformer weights\n",
        "        def _basic_init(module):\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "        self.apply(_basic_init)\n",
        "\n",
        "        # Initialize position embeddings\n",
        "        nn.init.normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"Convert the latent patches back into image space.\"\"\"\n",
        "        h = w = int(np.sqrt(x.shape[1]))\n",
        "        return x.reshape(shape=(x.shape[0], h, w, self.hidden_size))\n",
        "\n",
        "    def forward(self, x, timesteps, y):\n",
        "        \"\"\"\n",
        "        Forward pass of DiT.\n",
        "        x: (N, C, H, W) tensor of spatial inputs (images or latent representations)\n",
        "        timesteps: (N,) tensor of diffusion timesteps\n",
        "        y: (N,) tensor of class labels\n",
        "        \"\"\"\n",
        "        # 1. Input embedding and positioning\n",
        "        x = self.x_embedder(x)\n",
        "        x = x.flatten(2).transpose(1, 2)  # (N, T, C)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # 2. Time and class embeddings\n",
        "        t_emb = dit_block_mod.timestep_embedding(timesteps, self.hidden_size)\n",
        "        y_emb = dit_block_mod.label_embedding(y, self.num_classes, self.hidden_size)\n",
        "        c = t_emb + y_emb\n",
        "\n",
        "        # 3. DiT blocks\n",
        "        for block_params in self.blocks:\n",
        "            x = dit_block_mod.dit_block_forward(\n",
        "                x, c,\n",
        "                block_params['w_mod'], block_params['b_mod'],\n",
        "                block_params['wQ'], block_params['bQ'],\n",
        "                block_params['wK'], block_params['bK'],\n",
        "                block_params['wV'], block_params['bV'],\n",
        "                block_params['wO'], block_params['bO'],\n",
        "                block_params['w1'], block_params['b1'],\n",
        "                block_params['w2'], block_params['b2'],\n",
        "                eps=1e-6\n",
        "            )\n",
        "\n",
        "        # 4. Final layer norm and output projection\n",
        "        x = self.unpatchify(x)\n",
        "        x = dit_block_mod.final_layer_forward(x)\n",
        "\n",
        "        # 5. Split output into mean and variance if learning sigma\n",
        "        if self.learn_sigma:\n",
        "            x, log_var = x.chunk(2, dim=-1)\n",
        "            return x, log_var\n",
        "        return x, None\n",
        "\n",
        "def download_model(image_size):\n",
        "    \"\"\"Download pretrained DiT model if not exists\"\"\"\n",
        "    os.makedirs('pretrained', exist_ok=True)\n",
        "    model_path = f'pretrained/DiT-XL-2-{image_size}x{image_size}.pt'\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Downloading DiT-XL/2 {image_size}x{image_size} model...\")\n",
        "        urllib.request.urlretrieve(MODEL_URLS[str(image_size)], model_path)\n",
        "\n",
        "    return model_path\n",
        "\n",
        "def generate_images(\n",
        "    prompt=None,  # Text prompt for class-conditional generation\n",
        "    num_samples=1,\n",
        "    image_size=256,  # Choose 256 or 512\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=50,\n",
        "    seed=None,\n",
        "    device=\"cuda\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate images using the CUDA-optimized DiT model.\n",
        "\n",
        "    Args:\n",
        "        prompt (str or int, optional): Text prompt or ImageNet class index (0-999)\n",
        "        num_samples (int): Number of images to generate\n",
        "        image_size (int): Output image size (256 or 512)\n",
        "        guidance_scale (float): Classifier-free guidance scale\n",
        "        num_inference_steps (int): Number of diffusion steps\n",
        "        seed (int, optional): Random seed for reproducibility\n",
        "        device (str): Device to run on ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        list[PIL.Image]: Generated images\n",
        "    \"\"\"\n",
        "    assert image_size in [256, 512], \"Image size must be either 256 or 512\"\n",
        "\n",
        "    # Set random seed if provided\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    # Download and load pretrained model\n",
        "    model_path = download_model(image_size)\n",
        "    model = DiTInference(input_size=image_size//8)\n",
        "    state_dict = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(state_dict[\"model\"])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Convert prompt to class label if needed\n",
        "    if isinstance(prompt, str):\n",
        "        # TODO: Add text-to-class mapping for ImageNet\n",
        "        class_labels = torch.zeros(num_samples, dtype=torch.long, device=device)\n",
        "    elif isinstance(prompt, int):\n",
        "        class_labels = torch.full((num_samples,), prompt, dtype=torch.long, device=device)\n",
        "    else:\n",
        "        class_labels = torch.zeros(num_samples, dtype=torch.long, device=device)\n",
        "\n",
        "    # Create diffusion scheduler and VAE\n",
        "    diffusion = create_diffusion(str(num_inference_steps))\n",
        "    vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-ema\").to(device)\n",
        "\n",
        "    # Sample latents and generate images\n",
        "    with torch.no_grad():\n",
        "        # Initialize latents\n",
        "        latents = torch.randn(num_samples, 4, image_size//8, image_size//8, device=device)\n",
        "\n",
        "        # Sampling function with classifier-free guidance\n",
        "        def model_fn(x_t, t, y):\n",
        "            if guidance_scale == 1:\n",
        "                return model(x_t, t, y)[0]\n",
        "\n",
        "            # For classifier-free guidance, run both conditional and unconditional forward passes\n",
        "            x_in = torch.cat([x_t] * 2)\n",
        "            t_in = torch.cat([t] * 2)\n",
        "            y_in = torch.cat([y, torch.zeros_like(y)])\n",
        "\n",
        "            noise_pred, _ = model(x_in, t_in, y_in)\n",
        "            noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
        "            return noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
        "\n",
        "        # Run diffusion sampling\n",
        "        latents = diffusion.sample(model_fn, latents, class_labels)\n",
        "\n",
        "        # Decode latents to images\n",
        "        x = vae.decode(latents / 0.18215).sample\n",
        "        x = torch.clamp((x + 1) / 2, 0, 1)\n",
        "        x = (x * 255).round().to(torch.uint8)\n",
        "        images = x.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "        # Convert to PIL images\n",
        "        return [Image.fromarray(img) for img in images]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    images = generate_images(\n",
        "        prompt=\"A photo of a cat\",  # or use class index: prompt=281 for 'tabby cat'\n",
        "        num_samples=4,\n",
        "        image_size=256,\n",
        "        guidance_scale=7.5\n",
        "    )\n",
        "\n",
        "    # Save generated images\n",
        "    os.makedirs('outputs', exist_ok=True)\n",
        "    for i, image in enumerate(images):\n",
        "        image.save(f'outputs/sample_{i}.png')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torchevc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}