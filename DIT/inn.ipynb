{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3cee96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/satyam/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Load CUDA modules\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     dit_block_mod = \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdit_block_mod\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43msources\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/satyam/Music/DIT/src/cuda/dit_block.cu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/satyam/Music/DIT/src/cuda/cuda_adaln_modulation.cu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/satyam/Music/DIT/src/cuda/cuda_attention.cu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/satyam/Music/DIT/src/cuda/cuda_mlp.cu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/satyam/Music/DIT/src/cuda/cuda_timestep_embed.cu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/satyam/Music/DIT/src/cuda/cuda_util_kernels.cu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/satyam/Music/DIT/src/cuda/cuda_layernorm.cu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/satyam/Music/DIT/src/cuda/cuda_label_embed.cu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/satyam/Music/DIT/src/cuda/final_layer.cu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-O3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to load CUDA extensions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Music/torchevc/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1380\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[39m\n\u001b[32m   1288\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(name,\n\u001b[32m   1289\u001b[39m          sources: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[32m   1290\u001b[39m          extra_cflags=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1298\u001b[39m          is_standalone=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1299\u001b[39m          keep_intermediates=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1300\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1301\u001b[39m \u001b[33;03m    Load a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[32m   1302\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1378\u001b[39m \u001b[33;03m        ...     verbose=True)\u001b[39;00m\n\u001b[32m   1379\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1380\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jit_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43msources\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_get_build_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_python_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_intermediates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_intermediates\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Music/torchevc/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1815\u001b[39m, in \u001b[36m_jit_compile\u001b[39m\u001b[34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[39m\n\u001b[32m   1813\u001b[39m         baton.release()\n\u001b[32m   1814\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1815\u001b[39m     \u001b[43mbaton\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m   1818\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLoading extension module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m, file=sys.stderr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Music/torchevc/lib/python3.12/site-packages/torch/utils/file_baton.py:43\u001b[39m, in \u001b[36mFileBaton.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03mPeriodically sleeps for a certain amount until the baton is released.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03mThe amount of time slept depends on the ``wait_seconds`` parameter\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03mpassed to the constructor.\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m os.path.exists(\u001b[38;5;28mself\u001b[39m.lock_file_path):\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait_seconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Add the DiT directory to the Python path\n",
    "sys.path.insert(0, \"/home/satyam/Music/DIT/src/cuda/DiT\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.cpp_extension import load\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from diffusers.models import AutoencoderKL\n",
    "try:\n",
    "    from diffusion.scheduler import create_diffusion  # Adjust based on actual file\n",
    "except ImportError:\n",
    "    # Fallback to diffusers.DDPMScheduler if create_diffusion is unavailable\n",
    "    from diffusers import DDPMScheduler\n",
    "    def create_diffusion(timesteps):\n",
    "        return DDPMScheduler(num_train_timesteps=int(timesteps))\n",
    "import urllib.request\n",
    "\n",
    "# Pre-trained model URLs\n",
    "MODEL_URLS = {\n",
    "    '256': 'https://dl.fbaipublicfiles.com/DiT/models/DiT-XL-2-256x256.pt',\n",
    "    '512': 'https://dl.fbaipublicfiles.com/DiT/models/DiT-XL-2-512x512.pt'\n",
    "}\n",
    "\n",
    "# Load CUDA modules\n",
    "try:\n",
    "    dit_block_mod = load(\n",
    "        name=\"dit_block_mod\",\n",
    "        sources=[\n",
    "            \"/home/satyam/Music/DIT/src/cuda/dit_block.cu\",\n",
    "            \"/home/satyam/Music/DIT/src/cuda/cuda_adaln_modulation.cu\",\n",
    "            \"/home/satyam/Music/DIT/src/cuda/cuda_attention.cu\",\n",
    "            \"/home/satyam/Music/DIT/src/cuda/cuda_mlp.cu\",\n",
    "            \"/home/satyam/Music/DIT/src/cuda/cuda_timestep_embed.cu\",\n",
    "            \"/home/satyam/Music/DIT/src/cuda/cuda_util_kernels.cu\",\n",
    "            \"/home/satyam/Music/DIT/src/cuda/cuda_layernorm.cu\",\n",
    "            \"/home/satyam/Music/DIT/src/cuda/cuda_label_embed.cu\",\n",
    "            \"/home/satyam/Music/DIT/src/cuda/final_layer.cu\"\n",
    "        ],\n",
    "        extra_cuda_cflags=[\"-O3\"],\n",
    "        verbose=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load CUDA extensions: {e}\")\n",
    "    raise\n",
    "\n",
    "class DiTInference(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=32,  # 256/8 = 32 for 256x256 images\n",
    "        hidden_size=1152,  # DiT-XL/2 default\n",
    "        depth=28,  # DiT-XL/2 default\n",
    "        num_heads=16,  # DiT-XL/2 default\n",
    "        mlp_ratio=4.0,\n",
    "        num_classes=1000,\n",
    "        learn_sigma=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.learn_sigma = learn_sigma\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.depth = depth\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Input embedding\n",
    "        self.x_embedder = nn.Conv2d(4, hidden_size, kernel_size=1)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, input_size ** 2, hidden_size))\n",
    "\n",
    "        # Initialize weights for DiT blocks (loaded from checkpoint later)\n",
    "        self.blocks = []\n",
    "        for i in range(depth):\n",
    "            block_params = {\n",
    "                'hidden_size': hidden_size,\n",
    "                'num_heads': num_heads,\n",
    "                'mlp_ratio': mlp_ratio,\n",
    "                'qkv_bias': True\n",
    "            }\n",
    "            self.blocks.append(block_params)\n",
    "\n",
    "        # Final layer norm and output projection\n",
    "        self.final_layer = nn.Linear(hidden_size, 8 if learn_sigma else 4)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # Initialize transformer weights\n",
    "        def _basic_init(module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "        self.apply(_basic_init)\n",
    "\n",
    "        # Initialize position embeddings\n",
    "        nn.init.normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"Convert the latent patches back into image space.\"\"\"\n",
    "        h = w = int(np.sqrt(x.shape[1]))\n",
    "        return x.reshape(shape=(x.shape[0], h, w, self.hidden_size))\n",
    "\n",
    "    def forward(self, x, timesteps, y):\n",
    "        \"\"\"\n",
    "        Forward pass of DiT.\n",
    "        x: (N, C, H, W) tensor of spatial inputs (images or latent representations)\n",
    "        timesteps: (N,) tensor of diffusion timesteps\n",
    "        y: (N,) tensor of class labels\n",
    "        \"\"\"\n",
    "        # 1. Input embedding and positioning\n",
    "        x = self.x_embedder(x)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (N, T, C)\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # 2. Time and class embeddings\n",
    "        t_emb = dit_block_mod.timestep_embedding(timesteps, self.hidden_size)\n",
    "        y_emb = dit_block_mod.label_embedding(y, self.num_classes, self.hidden_size)\n",
    "        c = t_emb + y_emb\n",
    "\n",
    "        # 3. DiT blocks\n",
    "        for block_params in self.blocks:\n",
    "            x = dit_block_mod.dit_block_forward(\n",
    "                x, c,\n",
    "                block_params['w_mod'], block_params['b_mod'],\n",
    "                block_params['wQ'], block_params['bQ'],\n",
    "                block_params['wK'], block_params['bK'],\n",
    "                block_params['wV'], block_params['bV'],\n",
    "                block_params['wO'], block_params['bO'],\n",
    "                block_params['w1'], block_params['b1'],\n",
    "                block_params['w2'], block_params['b2'],\n",
    "                eps=1e-6\n",
    "            )\n",
    "\n",
    "        # 4. Final layer norm and output projection\n",
    "        x = self.unpatchify(x)\n",
    "        x = dit_block_mod.final_layer_forward(x)\n",
    "        \n",
    "        # 5. Split output into mean and variance if learning sigma\n",
    "        if self.learn_sigma:\n",
    "            x, log_var = x.chunk(2, dim=-1)\n",
    "            return x, log_var\n",
    "        return x, None\n",
    "\n",
    "def download_model(image_size):\n",
    "    \"\"\"Download pretrained DiT model if not exists\"\"\"\n",
    "    os.makedirs('pretrained', exist_ok=True)\n",
    "    model_path = f'pretrained/DiT-XL-2-{image_size}x{image_size}.pt'\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Downloading DiT-XL/2 {image_size}x{image_size} model...\")\n",
    "        urllib.request.urlretrieve(MODEL_URLS[str(image_size)], model_path)\n",
    "    \n",
    "    return model_path\n",
    "\n",
    "def generate_images(\n",
    "    prompt=None,  # Text prompt for class-conditional generation\n",
    "    num_samples=1,\n",
    "    image_size=256,  # Choose 256 or 512\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=50,\n",
    "    seed=None,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate images using the CUDA-optimized DiT model.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str or int, optional): Text prompt or ImageNet class index (0-999)\n",
    "        num_samples (int): Number of images to generate\n",
    "        image_size (int): Output image size (256 or 512)\n",
    "        guidance_scale (float): Classifier-free guidance scale\n",
    "        num_inference_steps (int): Number of diffusion steps\n",
    "        seed (int, optional): Random seed for reproducibility\n",
    "        device (str): Device to run on ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        list[PIL.Image]: Generated images\n",
    "    \"\"\"\n",
    "    assert image_size in [256, 512], \"Image size must be either 256 or 512\"\n",
    "    \n",
    "    # Set random seed if provided\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "    # Download and load pretrained model\n",
    "    model_path = download_model(image_size)\n",
    "    model = DiTInference(input_size=image_size//8)\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict[\"model\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert prompt to class label if needed\n",
    "    if isinstance(prompt, str):\n",
    "        # TODO: Add text-to-class mapping for ImageNet\n",
    "        class_labels = torch.zeros(num_samples, dtype=torch.long, device=device)\n",
    "    elif isinstance(prompt, int):\n",
    "        class_labels = torch.full((num_samples,), prompt, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        class_labels = torch.zeros(num_samples, dtype=torch.long, device=device)\n",
    "    \n",
    "    # Create diffusion scheduler and VAE\n",
    "    diffusion = create_diffusion(str(num_inference_steps))\n",
    "    vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-ema\").to(device)\n",
    "    \n",
    "    # Sample latents and generate images\n",
    "    with torch.no_grad():\n",
    "        # Initialize latents\n",
    "        latents = torch.randn(num_samples, 4, image_size//8, image_size//8, device=device)\n",
    "        \n",
    "        # Sampling function with classifier-free guidance\n",
    "        def model_fn(x_t, t, y):\n",
    "            if guidance_scale == 1:\n",
    "                return model(x_t, t, y)[0]\n",
    "            \n",
    "            # For classifier-free guidance, run both conditional and unconditional forward passes\n",
    "            x_in = torch.cat([x_t] * 2)\n",
    "            t_in = torch.cat([t] * 2)\n",
    "            y_in = torch.cat([y, torch.zeros_like(y)])\n",
    "            \n",
    "            noise_pred, _ = model(x_in, t_in, y_in)\n",
    "            noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "            return noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "        \n",
    "        # Run diffusion sampling\n",
    "        latents = diffusion.sample(model_fn, latents, class_labels)\n",
    "        \n",
    "        # Decode latents to images\n",
    "        x = vae.decode(latents / 0.18215).sample\n",
    "        x = torch.clamp((x + 1) / 2, 0, 1)\n",
    "        x = (x * 255).round().to(torch.uint8)\n",
    "        images = x.cpu().permute(0, 2, 3, 1).numpy()\n",
    "        \n",
    "        # Convert to PIL images\n",
    "        return [Image.fromarray(img) for img in images]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    images = generate_images(\n",
    "        prompt=\"A photo of a cat\",  # or use class index: prompt=281 for 'tabby cat'\n",
    "        num_samples=4,\n",
    "        image_size=256,\n",
    "        guidance_scale=7.5\n",
    "    )\n",
    "    \n",
    "    # Save generated images\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    for i, image in enumerate(images):\n",
    "        image.save(f'outputs/sample_{i}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchevc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
